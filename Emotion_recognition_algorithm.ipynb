{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy.signal as sig\n",
    "from biosppy.signals.bvp import bvp\n",
    "from biosppy.signals.eda import eda\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "import pickle\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.signal import butter, lfilter, iirnotch, welch\n",
    "\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Preparation\n",
    "\n",
    "![dataset_sampling_rates.png](Images/dataset_sampling_rates.png)\n",
    "\n",
    "Notes About Signal Preparation:\n",
    "- All Empatica Signals were upsampled to match the muse samples so that the data can be inputted into a CNN model consistently. This means that the sampling frequency used accross those signals is 256 Hz.\n",
    "- Samsung watch signals not included at the moment because they have a sampling rate that is inconsistent with the Muse Headset\n",
    "- Important modifications before inputing data to model:\n",
    "    - Make sure to center at zero\n",
    "    - Standardize data when appropriate (Such as for filtered EEG data)\n",
    "    - Use timestamps to allign to stimuli but do not input it into model\n",
    "    - Data augmentations to keep in mind: flipping the signal, making the overlap between signals larger\n",
    "    - Possibly downsampling all the signals so that it is easier for the CNN to find more low frequency patterns (as may be expected to exist in signals for emotion recognition)\n",
    "        - To this end, also if wanting to use power spectrum rather than raw EEG signal, this would make sense"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "with open(r\"C:\\Users\\nelso\\Desktop\\18065 Project Data\\Emognition\\22\\22_AMUSEMENT_STIMULUS_MUSE.json\") as json_file:\n",
    "    data = json.load(json_file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    '''\n",
    "    Performs bandpass filter on EEG data\n",
    "\n",
    "    data: array, of shape channels by samples\n",
    "    lowcut: int, the lower cut-off frequency\n",
    "    highcut: int, the higher cut-off frequency\n",
    "    fs: int, sampling frequency of the signal\n",
    "    order: int, the order of the butterworth filter\n",
    "\n",
    "    return: array, of same shape as data with filtered data\n",
    "    '''\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    y = lfilter(b, a, data)\n",
    "    return y\n",
    "\n",
    "def norm_stand_data(X,max_val=1,min_val=0):\n",
    "    '''\n",
    "    Standardizes and normalizes data\n",
    "\n",
    "    X: array, of shape trials by channels by samples\n",
    "    max_val: float, the maximum value to standardize data to\n",
    "    min_val: float, the minimum value to standardize data to\n",
    "\n",
    "    return: array, of the same shape as X but having standardized values\n",
    "    '''\n",
    "    X_std = (X - X.min(axis=1,keepdims=True)) / (X.max(axis=1,keepdims=True) - X.min(axis=1,keepdims=True))\n",
    "    X_scaled = X_std * (max_val - min_val) + min_val\n",
    "    return X_scaled"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.28718999, 0.28714483, 0.28709843, ..., 0.27497005, 0.2750411 ,\n       0.27511047])"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def eeg_analysis(eeg_data,l_freq=5,h_freq=55):\n",
    "    eeg_df = pd.DataFrame.from_dict(eeg_data)\n",
    "    eeg_timestamp = eeg_df.pop(\"TimeStamp\")\n",
    "    eeg_df = eeg_df.astype(\"float32\")\n",
    "    eeg_df.drop(['Delta_TP9', 'Delta_AF7', 'Delta_AF8', 'Delta_TP10', 'Theta_TP9',\n",
    "               'Theta_AF7', 'Theta_AF8', 'Theta_TP10', 'Alpha_TP9', 'Alpha_AF7',\n",
    "               'Alpha_AF8', 'Alpha_TP10', 'Beta_TP9', 'Beta_AF7', 'Beta_AF8',\n",
    "               'Beta_TP10', 'Gamma_TP9', 'Gamma_AF7', 'Gamma_AF8', 'Gamma_TP10'],axis=1,inplace=True)\n",
    "    eeg_df = eeg_df.interpolate()\n",
    "    eeg_df[\"TimeStamp\"] = eeg_timestamp\n",
    "    for eeg_chan in ['RAW_TP9', 'RAW_AF7', 'RAW_AF8', 'RAW_TP10']:\n",
    "        filtered_eeg = butter_bandpass_filter(eeg_df[eeg_chan].to_numpy(), l_freq, h_freq, 256, order=4)\n",
    "        # print(\"filtered_eeg shape: \", filtered_eeg.shape)\n",
    "        # print(filtered_eeg)\n",
    "        eeg_df[\"FILT_\"+eeg_chan.split(\"_\")[1]] = filtered_eeg\n",
    "    eeg_sig_len = len(eeg_df[:])\n",
    "    return eeg_df[:], eeg_sig_len\n",
    "\n",
    "def bvp_analysis(data,eeg_len,do_plot = False):\n",
    "    bvp_data = np.array(data['BVP'])[:,-1].astype('float32')\n",
    "    bvp_data = sig.resample(bvp_data,eeg_len)\n",
    "    _, _, _, _, heart_rate = bvp(bvp_data,256,show=do_plot)\n",
    "    heart_rate = sig.resample(heart_rate,eeg_len)\n",
    "\n",
    "    if do_plot:\n",
    "        plt.figure()\n",
    "        plt.plot([i/256 for i in range(len(heart_rate))],heart_rate)\n",
    "        plt.title(\"Heart Rate Interpolated\")\n",
    "    return bvp_data,heart_rate\n",
    "\n",
    "# EDA Signal Analysis\n",
    "def eda_analysis(data,eeg_len,do_plot = False):\n",
    "    eda_data = np.array(data['EDA'])[:,-1].astype('float32')\n",
    "\n",
    "    eda_data = sig.resample(eda_data,eeg_len)\n",
    "    _, filtered_eda, _, _, _ = eda(eda_data,256,show=do_plot)\n",
    "    return filtered_eda\n",
    "\n",
    "# TESTING\n",
    "\n",
    "# To test eeg_analysis data frame generation\n",
    "with open(r\"C:\\Users\\nelso\\Desktop\\18065 Project Data\\Emognition\\22\\22_AMUSEMENT_STIMULUS_MUSE.json\") as json_file:\n",
    "    eeg_data = json.load(json_file)\n",
    "    eeg_df, eeg_sig_len = eeg_analysis(eeg_data)\n",
    "\n",
    "# To test other physiolocial signals processing\n",
    "with open(r\"C:\\Users\\nelso\\Desktop\\18065 Project Data\\Emognition\\22\\22_AMUSEMENT_STIMULUS_EMPATICA.json\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "#BVP signal analysis:\n",
    "\n",
    "bvp_analysis(data,eeg_sig_len,True)\n",
    "\n",
    "eda_analysis(data,eeg_sig_len,True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "file_loc = r\"C:\\Users\\nelso\\Desktop\\18065 Project Data\\Emognition\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "emotions = [\"BASELINE\",\"NEUTRAL\",\"AMUSEMENT\",\"ANGER\",\"AWE\",\"DISGUST\",\"ENTHUSIASM\",\"FEAR\",\"LIKING\",\"SADNESS\",\"SURPRISE\"]\n",
    "class DeviceMissing(Exception):\n",
    "    \"\"\"Device Missing Exception\"\"\"\n",
    "    pass\n",
    "\n",
    "devices_of_interest = [\"MUSE\",\"EMPATICA\"] #MUSE must be loaded first!\n",
    "Testing = False\n",
    "per_user_labels = {}\n",
    "per_user_stimulus_arr = {}\n",
    "for root,dir,_ in os.walk(file_loc):\n",
    "    for d in dir:\n",
    "        participant = os.path.join(root,d)\n",
    "\n",
    "        #Map user to Label in Dicionary\n",
    "        emotion_dict = {e:{} for e in emotions}\n",
    "        labels_loc = participant + \"\\\\\" + d + \"_QUESTIONNAIRES.json\"\n",
    "        with open(labels_loc) as json_file:\n",
    "            quest_results = json.load(json_file)\n",
    "            for condition in quest_results['questionnaires']:\n",
    "                cond_type = condition[\"movie\"]\n",
    "                emotion_dict[cond_type] = condition[\"emotions\"]\n",
    "        per_user_labels[d] = emotion_dict\n",
    "        per_user_stimulus_arr[d] = {}\n",
    "\n",
    "        for emotion in emotions:\n",
    "            for device in devices_of_interest:\n",
    "                data_loc = participant + f\"\\\\{d}_{emotion}_STIMULUS_{device}.json\"\n",
    "\n",
    "                try:\n",
    "                    with open(data_loc) as json_file:\n",
    "                        data = json.load(json_file)\n",
    "                        if device == \"MUSE\":\n",
    "                            per_user_stimulus_arr[d][emotion],eeg_sig_len = eeg_analysis(data)\n",
    "                        elif device == \"EMPATICA\":\n",
    "                            bvp_data, heart_rate = bvp_analysis(data,eeg_sig_len,do_plot=Testing)\n",
    "                            per_user_stimulus_arr[d][emotion][\"BVP_Filt\"] = bvp_data\n",
    "                            per_user_stimulus_arr[d][emotion][\"HR_EMPATICA\"] = heart_rate\n",
    "                            filt_eda = eda_analysis(data,eeg_sig_len,do_plot = Testing)\n",
    "                            per_user_stimulus_arr[d][emotion][\"EDA_EMPATICA\"] = filt_eda\n",
    "                        else:\n",
    "                            raise DeviceMissing\n",
    "                except FileNotFoundError:\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    raise e"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "signals_of_interest = ['FILT_TP9', 'FILT_AF7', 'FILT_AF8', 'FILT_TP10','HR_EMPATICA','EDA_EMPATICA','BVP_Filt']\n",
    "rated_emotions = emotions[2:]\n",
    "\n",
    "window_len, overlap_len = 30*256, 20*256\n",
    "epoch_len = window_len - overlap_len\n",
    "target_size = 256*115\n",
    "all_sig_arr = np.zeros([1,target_size,len(signals_of_interest)])\n",
    "all_labels_arr = np.zeros([1,len(rated_emotions)])\n",
    "labels_arr_order = {e:i for i,e in enumerate(per_user_labels[\"22\"][\"BASELINE\"].keys())}\n",
    "subtracting_baseline = False\n",
    "\n",
    "for user in per_user_stimulus_arr:\n",
    "    user_data = per_user_stimulus_arr[user]\n",
    "    for affect in user_data:\n",
    "        if affect == \"BASELINE\":\n",
    "            continue #continue onto the next loop\n",
    "\n",
    "        #Get array of labels:\n",
    "        labels = np.zeros([1,len(rated_emotions)])\n",
    "        user_labels = per_user_labels[user]\n",
    "        for e in rated_emotions:\n",
    "            labels[0,labels_arr_order[e]] = user_labels[affect][e]\n",
    "        if subtracting_baseline:\n",
    "            pass\n",
    "\n",
    "        all_labels_arr = np.vstack([all_labels_arr,labels])\n",
    "\n",
    "        #Get signal arrays:\n",
    "\n",
    "        data_chunk = user_data[affect][signals_of_interest].to_numpy()[256:target_size+256]\n",
    "        data_len,num_chan = data_chunk.shape\n",
    "        # print(\"data_len before: \", data_len)\n",
    "        while data_len != target_size: #make all data the same by putting the beg of data at end\n",
    "            data_chunk = np.vstack([data_chunk,data_chunk[:target_size-data_len]])\n",
    "            data_len = data_chunk.shape[0]\n",
    "\n",
    "        all_sig_arr = np.vstack([all_sig_arr,data_chunk[np.newaxis,:,:]])\n",
    "\n",
    "#Epoching arrays:\n",
    "split_arr = [all_sig_arr[1:,start:start + window_len,:]\n",
    "             for start in range(0,data_len - window_len,epoch_len)]\n",
    "num_splits = len(split_arr)\n",
    "new_sig_arr = np.vstack(split_arr)\n",
    "new_labels_arr = np.vstack([all_labels_arr[1:,:] for _ in range(num_splits)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "#Save data arrays:\n",
    "np.save(r\"C:\\Users\\nelso\\Desktop\\18065 Project Data\\emognition_labels_arr.npy\",new_labels_arr)\n",
    "np.save(r\"C:\\Users\\nelso\\Desktop\\18065 Project Data\\emognition_signal_arr.npy\",new_sig_arr)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "#Load data array\n",
    "processed_arr = np.load(r\"C:\\Users\\nelso\\Desktop\\18065 Project Data\\emognition_signal_arr.npy\")\n",
    "processed_labels = np.load(r\"C:\\Users\\nelso\\Desktop\\18065 Project Data\\emognition_labels_arr.npy\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(processed_arr,processed_labels, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Picking Data Point 390\n",
      "(7680,)\n",
      "Picking Data Point 10\n",
      "(7680,)\n",
      "Picking Data Point 309\n",
      "(7680,)\n",
      "Picking Data Point 1038\n",
      "(7680,)\n",
      "Picking Data Point 2257\n",
      "(7680,)\n",
      "Picking Data Point 861\n",
      "(7680,)\n",
      "Picking Data Point 2860\n",
      "(7680,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nelso\\AppData\\Local\\Temp\\ipykernel_36672\\2266983084.py:13: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Picking Data Point 1917\n",
      "(7680,)\n",
      "Picking Data Point 1470\n",
      "(7680,)\n",
      "Picking Data Point 386\n",
      "(7680,)\n"
     ]
    }
   ],
   "source": [
    "#Testing Data Integrity:\n",
    "for _ in range(10):\n",
    "    i = np.random.randint(0,len(X_train))\n",
    "    print(f\"Picking Data Point {i}\")\n",
    "    eeg_data = X_train[i,:,:-3]\n",
    "    plt.figure()\n",
    "    plt.plot([i/256 for i in range(len(eeg_data))],eeg_data)\n",
    "    plt.title(\"EEG Spectrum Bands\")\n",
    "    bvp_data = X_train[i,:,np.array(signals_of_interest) == 'BVP_Filt'].flatten()\n",
    "    print(bvp_data.shape)\n",
    "    _, _, _, _, heart_rate = bvp(bvp_data,256,show=True)\n",
    "    eda_data = X_train[i,:,np.array(signals_of_interest) == 'EDA_EMPATICA'].flatten()\n",
    "    plt.figure()\n",
    "    plt.plot([i/256 for i in range(len(eda_data))],eda_data)\n",
    "    plt.title(\"EDA Data Plot\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "data": {
      "text/plain": "(2880, 7680, 7)"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "(2880, 7680, 6)"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "(2880, 9)"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "(720, 7680, 6)"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "(720, 9)"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "with open(r\"C:\\Users\\nelso\\Desktop\\18065 Project Data\\Emognition\\22\\22_BASELINE_STIMULUS_MUSE.json\") as json_file:\n",
    "    eeg_data = json.load(json_file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered_eeg shape:  (30739,)\n",
      "[ 35.15819602 189.76799187 431.44915043 ...  -1.8485881   -1.38779581\n",
      "   1.24229346]\n",
      "filtered_eeg shape:  (30739,)\n",
      "[ 34.33975295 185.97486991 425.18208225 ...   6.48738635   3.89985449\n",
      "   1.91576813]\n",
      "filtered_eeg shape:  (30739,)\n",
      "[ 3.46009575e+01  1.86950169e+02  4.25064290e+02 ... -6.36177604e+00\n",
      " -4.16509376e+00  7.10090294e-02]\n",
      "filtered_eeg shape:  (30739,)\n",
      "[ 34.89699146 188.70562466 430.05191096 ...  -8.08749227  -5.40383918\n",
      "  -1.55377611]\n",
      "filtered_eeg shape:  (30739,)\n",
      "[ 35.15819602 189.76799187 431.44915043 ...  -1.8485881   -1.38779581\n",
      "   1.24229346]\n",
      "filtered_eeg shape:  (30739,)\n",
      "[ 34.33975295 185.97486991 425.18208225 ...   6.48738635   3.89985449\n",
      "   1.91576813]\n",
      "filtered_eeg shape:  (30739,)\n",
      "[ 3.46009575e+01  1.86950169e+02  4.25064290e+02 ... -6.36177604e+00\n",
      " -4.16509376e+00  7.10090294e-02]\n",
      "filtered_eeg shape:  (30739,)\n",
      "[ 34.89699146 188.70562466 430.05191096 ...  -8.08749227  -5.40383918\n",
      "  -1.55377611]\n",
      "filtered_eeg shape:  (30739,)\n",
      "[ 35.15819602 189.76799187 431.44915043 ...  -1.8485881   -1.38779581\n",
      "   1.24229346]\n",
      "filtered_eeg shape:  (30739,)\n",
      "[ 34.33975295 185.97486991 425.18208225 ...   6.48738635   3.89985449\n",
      "   1.91576813]\n",
      "filtered_eeg shape:  (30739,)\n",
      "[ 3.46009575e+01  1.86950169e+02  4.25064290e+02 ... -6.36177604e+00\n",
      " -4.16509376e+00  7.10090294e-02]\n",
      "filtered_eeg shape:  (30739,)\n",
      "[ 34.89699146 188.70562466 430.05191096 ...  -8.08749227  -5.40383918\n",
      "  -1.55377611]\n",
      "filtered_eeg shape:  (30739,)\n",
      "[ 35.15819602 189.76799187 431.44915043 ...  -1.8485881   -1.38779581\n",
      "   1.24229346]\n",
      "filtered_eeg shape:  (30739,)\n",
      "[ 34.33975295 185.97486991 425.18208225 ...   6.48738635   3.89985449\n",
      "   1.91576813]\n",
      "filtered_eeg shape:  (30739,)\n",
      "[ 3.46009575e+01  1.86950169e+02  4.25064290e+02 ... -6.36177604e+00\n",
      " -4.16509376e+00  7.10090294e-02]\n",
      "filtered_eeg shape:  (30739,)\n",
      "[ 34.89699146 188.70562466 430.05191096 ...  -8.08749227  -5.40383918\n",
      "  -1.55377611]\n"
     ]
    }
   ],
   "source": [
    "for eeg_chan in ['FILT_TP9', 'FILT_AF7', 'FILT_AF8', 'FILT_TP10']:\n",
    "    filtered_eeg = eeg_analysis(eeg_data)[0][eeg_chan]\n",
    "    plt.figure()\n",
    "    plt.plot([i/256 for i in range(len(eeg_df))],filtered_eeg)\n",
    "    plt.title(f\"EEG RAW Data {eeg_chan}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "eeg_df = pd.DataFrame.from_dict(eeg_data)\n",
    "eeg_df.columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "outputs": [],
   "source": [
    "for eeg_chan in ['RAW_TP9', 'RAW_AF7', 'RAW_AF8', 'RAW_TP10']:\n",
    "    plt.figure()\n",
    "    filtered_eeg = butter_bandpass_filter(eeg_df[eeg_chan], 5, 55, 256, order=4)\n",
    "    plt.plot([i/256 for i in range(len(eeg_df))],filtered_eeg)\n",
    "    plt.title(f\"EEG RAW Data {eeg_chan}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples:  691.2\n",
      "Training with augmentation:  1382.4\n",
      "Testing samples:  76.80000000000001\n"
     ]
    }
   ],
   "source": [
    "window = 30\n",
    "seconds_of_data = window - 20 #window minus overlap\n",
    "print(\"Training samples: \",2*60*256/(256*seconds_of_data)*64*0.9)\n",
    "print(\"Training with augmentation: \", 2*2*60*256/(256*seconds_of_data)*64*0.9)\n",
    "print(\"Testing samples: \",2*60*256/(256*seconds_of_data)*64*0.1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Calculations to ensure sampling rate and time of recordings match\n",
    "approx_samp_rate_subject20 = 64*(30739/7680)\n",
    "approx_exp_duration_subject20 = 30739/256/60"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Activation, Permute, Dropout\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, AveragePooling1D, MaxPooling1D\n",
    "from tensorflow.keras.layers import SeparableConv2D, DepthwiseConv2D, DepthwiseConv1D, SeparableConv1D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import SpatialDropout2D\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow.keras.layers import Input, Flatten\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow.keras import backend as K"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def SSVEPNet(nb_classes, Chans = 8, Samples = 256,\n",
    "             dropoutRate = 0.5, kernLength = 64,\n",
    "             D = 1, F2 = 16, norm_rate = 0.25, dropoutType = 'Dropout'):\n",
    "    \"\"\"\n",
    "    SSVEPNet is a convolutional neural network that uses depthwise and separable\n",
    "    convolutions to predict SSVEP signals\n",
    "\n",
    "    nb_classes: int,number of classes that will be predicted\n",
    "    Chans: int, the number of channels of EEG data\n",
    "    Samples: int, the number of samples per EEG signal\n",
    "    dropoutRate: foat, the dropout rate before the flattening layer\n",
    "    kernelLength: int, the length of the 1D convolutions\n",
    "    D: int, specifies the depth multiplier for first 1D convolution\n",
    "    F2: int, number of filters in the second layer\n",
    "    norm_rate: float, the normalization rate\n",
    "    dropoutType: str, type of Dropout\n",
    "\n",
    "    return: The Model built with tensorflow\n",
    "    \"\"\"\n",
    "\n",
    "    if dropoutType == 'SpatialDropout2D':\n",
    "        dropoutType = SpatialDropout2D\n",
    "    elif dropoutType == 'Dropout':\n",
    "        dropoutType = Dropout\n",
    "    else:\n",
    "        raise ValueError('dropoutType must be one of SpatialDropout2D '\n",
    "                         'or Dropout, passed as a string.')\n",
    "\n",
    "    input1   = Input(shape = (Samples, Chans))\n",
    "    block1       = DepthwiseConv1D(kernLength, use_bias = False,\n",
    "                                   depth_multiplier = D,padding='same')(input1)\n",
    "    block2       = SeparableConv1D(F2,kernLength,\n",
    "                                   use_bias = False, padding = 'same')(block1)\n",
    "    block2       = BatchNormalization()(block2)\n",
    "    block2       = Activation('elu')(block2)\n",
    "    block2       = AveragePooling1D(4)(block2)\n",
    "    block2       = SeparableConv1D(F2,int(kernLength/2),\n",
    "                                   use_bias = False, padding = 'same')(block2)\n",
    "    block2       = BatchNormalization()(block2)\n",
    "    block2       = Activation('elu')(block2)\n",
    "    block2       = AveragePooling1D(4)(block2)\n",
    "    block2       = dropoutType(dropoutRate)(block2)\n",
    "\n",
    "    flatten      = Flatten(name = 'flatten')(block2)\n",
    "\n",
    "    dense        = Dense(nb_classes, name = 'dense',\n",
    "                         kernel_constraint = max_norm(norm_rate))(flatten)\n",
    "    softmax      = Activation('softmax', name = 'softmax')(dense)\n",
    "\n",
    "    return Model(inputs=input1, outputs=softmax)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}